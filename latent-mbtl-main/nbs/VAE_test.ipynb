{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMY9xwL8+qWzMXIXlAQwIVS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukliio/Autoencoder-Driven-Model-Based-Transfer/blob/main/nbs/VAE_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is an autoencoder?"
      ],
      "metadata": {
        "id": "JWjncTfcJFP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "metadata": {
        "id": "v8QrWFAgJPyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the MNIST dataset\n",
        "\n",
        "mnist_train = torchvision.datasets.MNIST(\n",
        "    root='./',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "mnist_test = torchvision.datasets.MNIST(\n",
        "    root='./',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "gV4l2-dDJUEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of training samples:\", len(mnist_train))\n",
        "\n",
        "loader = DataLoader(mnist_train, batch_size=len(mnist_train))\n",
        "images, ground_truth = next(iter(loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sA-BD1SLzBC",
        "outputId": "f0f3f1d2-b9a1-44d2-ab6d-85fe03a383ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 60000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images = images.squeeze()  # removes dimensions of size (1)\n",
        "print(images.shape)\n",
        "print(ground_truth.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PWaNtEWaOj99",
        "outputId": "a1f5cb63-3447-4a73-ae5b-161b4a7b3d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([60000, 28, 28])\n",
            "torch.Size([60000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.min(images[0]))\n",
        "print(torch.max(images[0]))\n",
        "\n",
        "images_int = (images * 255).byte()  # convert to 0-255\n",
        "print(images_int.min(), images_int.max())  # 0 255"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZJPYVVyQ1My",
        "outputId": "155b0c34-eb06-4ca4-de8c-c4c2fc6d3923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.)\n",
            "tensor(1.)\n",
            "tensor(0, dtype=torch.uint8) tensor(255, dtype=torch.uint8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_index = 15\n",
        "\n",
        "print(ground_truth[image_index])\n",
        "plt.imshow(images[image_index], cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "B70yaNeERQV9",
        "outputId": "c779c072-6fd1-4f31-d7f1-f0c70eac04c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x797e88145f10>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "mnist_train = datasets.MNIST(\n",
        "    root='./',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "\n",
        "# stack all images into one tensor\n",
        "all_images = torch.stack([img for img, label in mnist_train])  # [60000, 1, 28, 28]\n",
        "\n",
        "# stack all labels into one tensor\n",
        "all_labels = torch.tensor([label for img, label in mnist_train])  # [60000]\n",
        "\n",
        "# save to file\n",
        "torch.save((all_images, all_labels), 'mnist_train.pt')\n",
        "print(\"File saved as mnist_train.pt\")"
      ],
      "metadata": {
        "id": "hRCuUqcNZtuM",
        "outputId": "f01ca209-85cc-42dc-f6dc-250873ed9a12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved as mnist_train.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mnist_test = datasets.MNIST(\n",
        "    root='./',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "\n",
        "# stack all images into one tensor\n",
        "all_images = torch.stack([img for img, label in mnist_test])\n",
        "\n",
        "# stack all labels into one tensor\n",
        "all_labels = torch.tensor([label for img, label in mnist_test])\n",
        "\n",
        "# save to file\n",
        "torch.save((all_images, all_labels), 'mnist_test.pt')\n",
        "print(\"File saved as mnist_test.pt\")"
      ],
      "metadata": {
        "id": "8kZsLEZrZ6V2",
        "outputId": "0d8af48e-cd75-4d6f-bcec-7dc82e322797",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved as mnist_test.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# map style pytorch dataset\n",
        "\n",
        "class MNISTDataset(Dataset):\n",
        "  # need to implement getitem and len methods. creates a map dataste and can query the dataset via index\n",
        "  def __init__(self, path):\n",
        "    # loads data into memory\n",
        "    self.images, self.ground_truth = torch.load(path)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # returns the image with index idx\n",
        "    x = self.images[idx].float()\n",
        "    x = torch.flatten(x)\n",
        "    y = self.ground_truth[idx]\n",
        "\n",
        "    return x, y\n",
        "\n",
        "  def __len__(self):\n",
        "  # returns the length of the dataset\n",
        "    return len(self.ground_truth)\n",
        "\n"
      ],
      "metadata": {
        "id": "AYYyhWyATElp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make the dataset\n",
        "train_dataset = MNISTDataset(\"mnist_train.pt\")\n",
        "test_dataset = MNISTDataset(\"mnist_test.pt\")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "-ua0xV2QZ3r_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "metadata": {
        "id": "-gjmPF7jOb0s",
        "outputId": "fb21b464-eb7a-4644-bb40-6eed7cbc8845",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n",
            "60000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = train_dataset[12]\n",
        "plt.imshow(x.numpy().reshape(28,28), cmap='gray')"
      ],
      "metadata": {
        "id": "vvpZliS1Q5W6",
        "outputId": "5cdba376-115c-46f5-e9fb-6e5f16757510",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x797e8b715bb0>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i0MNRcYGQNEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "  \"\"\"\n",
        "  params:\n",
        "    - input_shape: shape fo the input data\n",
        "    - latent_dim: size of the latent dim\n",
        "  \"\"\"\n",
        "  def __init__(self, input_shape, latent_dim=128):\n",
        "    super().__init__()\n",
        "    # two fully connected layers for encoder (input_shape --> latent dim)\n",
        "    self.encoder_11 = nn.Linear(in_features=input_shape, out_features=latent_dim)\n",
        "    self.encoder_12 = nn.Linear(in_features=latent_dim, out_features=latent_dim)\n",
        "\n",
        "    # two fully connected layers for decder (128 --> input_shape)\n",
        "    self.decoder_11 = nn.Linear(in_features=latent_dim, out_features=latent_dim)\n",
        "    self.decoder_12 = nn.Linear(in_features=latent_dim, out_features=input_shape)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    params:\n",
        "      - x: input data\n",
        "\n",
        "    returns:\n",
        "      - x_hat: reconstructed data\n",
        "    \"\"\"\n",
        "\n",
        "    latent = self.run_encoder(x)\n",
        "    x_hat = self.run_decoder(latent)\n",
        "    return x_hat\n",
        "\n",
        "  def run_encoder(self, x):\n",
        "    # runs encoder\n",
        "    output = F.relu(self.encoder_11(x))\n",
        "    latent = F.relu(self.encoder_12(output))\n",
        "    return latent\n",
        "\n",
        "  def run_decoder(self, latent):\n",
        "    # runs decoder\n",
        "    output = F.relu(self.decoder_11(latent))\n",
        "    x_hat = F.relu(self.decoder_12(output))\n",
        "    return x_hat\n"
      ],
      "metadata": {
        "id": "PGufqr3pQPby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = train_dataset[0]\n",
        "image_shape = x.shape[0]\n",
        "print(image_shape)\n",
        "model = AutoEncoder(input_shape=image_shape)\n",
        "model.train()\n",
        "model.cuda()\n"
      ],
      "metadata": {
        "id": "8iNP4rM-UswK",
        "outputId": "4047f288-b034-428b-dc0e-5d0be94fd4e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "784\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AutoEncoder(\n",
              "  (encoder_11): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (encoder_12): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (decoder_11): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (decoder_12): Linear(in_features=128, out_features=784, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TgnsR3aUzSGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "mae = nn.MSELoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "n_epochs = 100\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    loss = 0\n",
        "    for batch_features, _ in train_loader:\n",
        "        batch_features = batch_features.cuda()\n",
        "\n",
        "        # reset gradients back to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        outputs = model(batch_features)\n",
        "        train_loss = mae(outputs, batch_features)\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss += train_loss.item()\n",
        "\n",
        "    loss = loss / len(train_loader)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        print(\"epoch: {}/{}, loss = {:.6f}\".format(epoch + 1, n_epochs, loss))\n",
        "\n",
        "# final report\n",
        "print(\"epoch: {}/{}, loss = {:.6f}\".format(n_epochs, n_epochs, loss))\n",
        "\n"
      ],
      "metadata": {
        "id": "Y-L3i0QgVWco",
        "outputId": "521e4dc6-f9b6-4ab8-da19-046942b42104",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1/100, loss = 0.028639\n",
            "epoch: 6/100, loss = 0.014197\n",
            "epoch: 11/100, loss = 0.013376\n",
            "epoch: 16/100, loss = 0.012935\n",
            "epoch: 21/100, loss = 0.012675\n",
            "epoch: 26/100, loss = 0.012546\n",
            "epoch: 31/100, loss = 0.012486\n",
            "epoch: 36/100, loss = 0.012436\n",
            "epoch: 41/100, loss = 0.012397\n",
            "epoch: 46/100, loss = 0.012357\n",
            "epoch: 51/100, loss = 0.012330\n",
            "epoch: 56/100, loss = 0.012302\n",
            "epoch: 61/100, loss = 0.012290\n",
            "epoch: 66/100, loss = 0.012281\n",
            "epoch: 71/100, loss = 0.012238\n",
            "epoch: 76/100, loss = 0.012225\n",
            "epoch: 81/100, loss = 0.012214\n",
            "epoch: 86/100, loss = 0.012194\n",
            "epoch: 91/100, loss = 0.012186\n",
            "epoch: 96/100, loss = 0.012174\n",
            "epoch: 100/100, loss = 0.012160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "KFc8QnLX2MC0",
        "outputId": "eb5e8440-8b50-4427-eb62-cf1434f1038e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AutoEncoder(\n",
              "  (encoder_11): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (encoder_12): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (decoder_11): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (decoder_12): Linear(in_features=128, out_features=784, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kernel_size = 4\n",
        "init_channels = 8\n",
        "image_channels = 1\n",
        "latent_dim =16"
      ],
      "metadata": {
        "id": "PIC9bGwvqPDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "9tDYucmnqqZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a Conv VAE\n",
        "class ConvVAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvVAE, self).__init__()\n",
        "\n",
        "        # encoder\n",
        "        self.enc1 = nn.Conv2d(\n",
        "            in_channels=image_channels, out_channels=init_channels, kernel_size=kernel_size,\n",
        "            stride=2, padding=1\n",
        "        )\n",
        "        self.enc2 = nn.Conv2d(\n",
        "            in_channels=init_channels, out_channels=init_channels*2, kernel_size=kernel_size,\n",
        "            stride=2, padding=1\n",
        "        )\n",
        "        self.enc3 = nn.Conv2d(\n",
        "            in_channels=init_channels*2, out_channels=init_channels*4, kernel_size=kernel_size,\n",
        "            stride=2, padding=1\n",
        "        )\n",
        "        self.enc4 = nn.Conv2d(\n",
        "            in_channels=init_channels*4, out_channels=64, kernel_size=kernel_size,\n",
        "            stride=2, padding=0\n",
        "        )\n",
        "        # fully connected layers for learning representations\n",
        "        self.fc1 = nn.Linear(64, 128)\n",
        "        self.fc_mu = nn.Linear(128, latent_dim)\n",
        "        self.fc_log_var = nn.Linear(128, latent_dim)\n",
        "        self.fc2 = nn.Linear(latent_dim, 64)\n",
        "        # decoder\n",
        "        self.dec1 = nn.ConvTranspose2d(\n",
        "            in_channels=64, out_channels=init_channels*8, kernel_size=kernel_size,\n",
        "            stride=1, padding=0\n",
        "        )\n",
        "        self.dec2 = nn.ConvTranspose2d(\n",
        "            in_channels=init_channels*8, out_channels=init_channels*4, kernel_size=kernel_size,\n",
        "            stride=2, padding=1\n",
        "        )\n",
        "        self.dec3 = nn.ConvTranspose2d(\n",
        "            in_channels=init_channels*4, out_channels=init_channels*2, kernel_size=kernel_size,\n",
        "            stride=2, padding=1\n",
        "        )\n",
        "        self.dec4 = nn.ConvTranspose2d(\n",
        "            in_channels=init_channels*2, out_channels=image_channels, kernel_size=kernel_size,\n",
        "            stride=2, padding=1\n",
        "        )\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        \"\"\"\n",
        "        :param mu: mean from the encoder's latent space\n",
        "        :param log_var: log variance from the encoder's latent space\n",
        "        \"\"\"\n",
        "        std = torch.exp(0.5*log_var) # standard deviation\n",
        "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
        "        sample = mu + (eps * std) # sampling\n",
        "        return sample\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encoding\n",
        "        x = F.relu(self.enc1(x))\n",
        "        x = F.relu(self.enc2(x))\n",
        "        x = F.relu(self.enc3(x))\n",
        "        x = F.relu(self.enc4(x))\n",
        "        batch, _, _, _ = x.shape\n",
        "        x = F.adaptive_avg_pool2d(x, 1).reshape(batch, -1)\n",
        "        hidden = self.fc1(x)\n",
        "        # get `mu` and `log_var`\n",
        "        mu = self.fc_mu(hidden)\n",
        "        log_var = self.fc_log_var(hidden)\n",
        "        # get the latent vector through reparameterization\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        z = self.fc2(z)\n",
        "        z = z.view(-1, 64, 1, 1)\n",
        "\n",
        "        # decoding\n",
        "        x = F.relu(self.dec1(z))\n",
        "        x = F.relu(self.dec2(x))\n",
        "        x = F.relu(self.dec3(x))\n",
        "        reconstruction = torch.sigmoid(self.dec4(x))\n",
        "        return reconstruction, mu, log_var"
      ],
      "metadata": {
        "id": "eNt9hY981-uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"../outputs\", exist_ok=True)"
      ],
      "metadata": {
        "id": "A-2fJRYDsf_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import save_image\n",
        "to_pil_image = transforms.ToPILImage()\n",
        "def image_to_vid(images):\n",
        "    imgs = [np.array(to_pil_image(img)) for img in images]\n",
        "    imageio.mimsave('../outputs/generated_images.gif', imgs)\n",
        "def save_reconstructed_images(recon_images, epoch):\n",
        "    save_image(recon_images.cpu(), f\"../outputs/output{epoch}.jpg\")\n",
        "def save_loss_plot(train_loss, valid_loss):\n",
        "    # loss plots\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.plot(train_loss, color='orange', label='train loss')\n",
        "    plt.plot(valid_loss, color='red', label='validataion loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig('../outputs/loss.jpg')\n",
        "    plt.show()\n",
        "def final_loss(bce_loss, mu, logvar):\n",
        "    \"\"\"\n",
        "    This function will add the reconstruction loss (BCELoss) and the\n",
        "    KL-Divergence.\n",
        "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    :param bce_loss: recontruction loss\n",
        "    :param mu: the mean from the latent vector\n",
        "    :param logvar: log variance from the latent vector\n",
        "    \"\"\"\n",
        "    BCE = bce_loss\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n"
      ],
      "metadata": {
        "id": "jyNOgwWvqln0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, dataset, device, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    counter = 0\n",
        "    for i, data in tqdm(enumerate(dataloader), total=int(len(dataset)/dataloader.batch_size)):\n",
        "        counter += 1\n",
        "        data = data[0]\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        reconstruction, mu, logvar = model(data)\n",
        "        bce_loss = criterion(reconstruction, data)\n",
        "        loss = final_loss(bce_loss, mu, logvar)\n",
        "        loss.backward()\n",
        "        running_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    train_loss = running_loss / counter\n",
        "    return train_loss\n",
        "\n",
        "def validate(model, dataloader, dataset, device, criterion):\n",
        "  model.eval()\n",
        "  running_loss = 0.0\n",
        "  counter = 0\n",
        "  with torch.no_grad():\n",
        "      for i, data in tqdm(enumerate(dataloader), total=int(len(dataset)/dataloader.batch_size)):\n",
        "          counter += 1\n",
        "          data= data[0]\n",
        "          data = data.to(device)\n",
        "          reconstruction, mu, logvar = model(data)\n",
        "          bce_loss = criterion(reconstruction, data)\n",
        "          loss = final_loss(bce_loss, mu, logvar)\n",
        "          running_loss += loss.item()\n",
        "\n",
        "          # save the last batch input and output of every epoch\n",
        "          if i == int(len(dataset)/dataloader.batch_size) - 1:\n",
        "              recon_images = reconstruction\n",
        "  val_loss = running_loss / counter\n",
        "  return val_loss, recon_images"
      ],
      "metadata": {
        "id": "Z84ducxYq_tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import matplotlib\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "matplotlib.style.use('ggplot')\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# initialize the model\n",
        "model = ConvVAE().to(device)\n",
        "# set the learning parameters\n",
        "lr = 0.001\n",
        "epochs = 100\n",
        "batch_size = 64\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.BCELoss(reduction='sum')\n",
        "# a list to save all the reconstructed images in PyTorch grid format\n",
        "grid_images = []\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "# training set and train data loader\n",
        "trainset = torchvision.datasets.MNIST(\n",
        "    root='../input', train=True, download=True, transform=transform\n",
        ")\n",
        "trainloader = DataLoader(\n",
        "    trainset, batch_size=batch_size, shuffle=True\n",
        ")\n",
        "# validation set and validation data loader\n",
        "testset = torchvision.datasets.MNIST(\n",
        "    root='../input', train=False, download=True, transform=transform\n",
        ")\n",
        "testloader = DataLoader(\n",
        "    testset, batch_size=batch_size, shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "C1zZUk_1rVW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = []\n",
        "valid_loss = []\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
        "    train_epoch_loss = train(\n",
        "        model, trainloader, trainset, device, optimizer, criterion\n",
        "    )\n",
        "    valid_epoch_loss, recon_images = validate(\n",
        "        model, testloader, testset, device, criterion\n",
        "    )\n",
        "    train_loss.append(train_epoch_loss)\n",
        "    valid_loss.append(valid_epoch_loss)\n",
        "    # save the reconstructed images from the validation loop\n",
        "    save_reconstructed_images(recon_images, epoch+1)\n",
        "    # convert the reconstructed images to PyTorch image grid format\n",
        "    image_grid = make_grid(recon_images.detach().cpu())\n",
        "    grid_images.append(image_grid)\n",
        "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
        "    print(f\"Val Loss: {valid_epoch_loss:.4f}\")"
      ],
      "metadata": {
        "id": "U4ue5OcwrzBO",
        "outputId": "66c781ff-543f-4df9-98b8-bc9d031fd4be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 67.07it/s]\n",
            "157it [00:01, 99.24it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 14744.2218\n",
            "Val Loss: 11363.7844\n",
            "Epoch 2 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:14, 65.09it/s]                         \n",
            "157it [00:01, 95.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 11129.6455\n",
            "Val Loss: 10828.4508\n",
            "Epoch 3 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 67.29it/s]                         \n",
            "157it [00:01, 99.16it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 10805.4842\n",
            "Val Loss: 10676.1088\n",
            "Epoch 4 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 67.80it/s]                         \n",
            "157it [00:01, 99.21it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 10652.9855\n",
            "Val Loss: 10503.7485\n",
            "Epoch 5 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 67.14it/s]                         \n",
            "157it [00:02, 75.97it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 10545.3388\n",
            "Val Loss: 10450.1387\n",
            "Epoch 6 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 67.77it/s]                         \n",
            "157it [00:01, 98.68it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 10474.7405\n",
            "Val Loss: 10380.6490\n",
            "Epoch 7 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 68.38it/s]\n",
            "157it [00:01, 98.42it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 10391.4365\n",
            "Val Loss: 10292.7751\n",
            "Epoch 8 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 67.59it/s]                         \n",
            "157it [00:01, 81.90it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 10260.9256\n",
            "Val Loss: 10118.4801\n",
            "Epoch 9 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:14, 66.83it/s]                         \n",
            "157it [00:01, 100.12it/s]                        \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 10153.4863\n",
            "Val Loss: 10046.6554\n",
            "Epoch 10 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 67.55it/s]\n",
            "157it [00:01, 97.40it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 10105.1173\n",
            "Val Loss: 10038.5681\n",
            "Epoch 11 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 67.57it/s]\n",
            "157it [00:01, 96.78it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 10059.0595\n",
            "Val Loss: 9976.1646\n",
            "Epoch 12 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:14, 65.24it/s]\n",
            "157it [00:01, 96.21it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 10030.0394\n",
            "Val Loss: 9978.5816\n",
            "Epoch 13 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 68.26it/s]                         \n",
            "157it [00:01, 99.52it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 10004.2404\n",
            "Val Loss: 9954.4294\n",
            "Epoch 14 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 67.66it/s]                         \n",
            "157it [00:01, 98.47it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 9978.8923\n",
            "Val Loss: 9935.1939\n",
            "Epoch 15 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 67.13it/s]\n",
            "157it [00:02, 72.40it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 9961.1738\n",
            "Val Loss: 9909.5835\n",
            "Epoch 16 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 67.63it/s]                         \n",
            "157it [00:01, 99.11it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 9946.3839\n",
            "Val Loss: 9947.8771\n",
            "Epoch 17 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 68.08it/s]                         \n",
            "157it [00:01, 99.38it/s]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 9930.7502\n",
            "Val Loss: 9885.7724\n",
            "Epoch 18 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 67.31it/s]                         \n",
            "157it [00:01, 87.74it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 9912.7948\n",
            "Val Loss: 9883.6571\n",
            "Epoch 19 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:14, 65.22it/s]                         \n",
            "157it [00:01, 97.46it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 9897.2975\n",
            "Val Loss: 9870.1355\n",
            "Epoch 20 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 67.53it/s]                         \n",
            "157it [00:01, 97.67it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 9888.5814\n",
            "Val Loss: 9866.6482\n",
            "Epoch 21 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 67.84it/s]                         \n",
            "157it [00:01, 98.93it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 9875.0266\n",
            "Val Loss: 9843.1797\n",
            "Epoch 22 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:14, 65.16it/s]                         \n",
            "157it [00:01, 93.18it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 9865.5395\n",
            "Val Loss: 9822.0330\n",
            "Epoch 23 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 68.09it/s]                         \n",
            "157it [00:01, 98.57it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 9853.5900\n",
            "Val Loss: 9939.0263\n",
            "Epoch 24 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 68.01it/s]                         \n",
            "157it [00:01, 97.58it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 9822.8849\n",
            "Val Loss: 9774.6098\n",
            "Epoch 25 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 68.12it/s]                         \n",
            "157it [00:02, 68.50it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 9776.3640\n",
            "Val Loss: 9724.7097\n",
            "Epoch 26 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 67.89it/s]                         \n",
            "157it [00:01, 98.21it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 9729.7735\n",
            "Val Loss: 9705.8859\n",
            "Epoch 27 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 67.92it/s]                         \n",
            "157it [00:01, 98.39it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 9708.3297\n",
            "Val Loss: 9719.2115\n",
            "Epoch 28 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 67.88it/s]\n",
            "157it [00:01, 87.10it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 9691.8629\n",
            "Val Loss: 9661.1462\n",
            "Epoch 29 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:14, 66.31it/s]                         \n",
            "157it [00:01, 100.08it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 9680.1730\n",
            "Val Loss: 9649.4401\n",
            "Epoch 30 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 67.74it/s]                         \n",
            "157it [00:01, 98.66it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 9670.9064\n",
            "Val Loss: 9658.7910\n",
            "Epoch 31 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 67.58it/s]                         \n",
            "157it [00:01, 99.45it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 9663.0421\n",
            "Val Loss: 9654.5492\n",
            "Epoch 32 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:14, 66.27it/s]                         \n",
            "157it [00:01, 83.41it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 9654.0268\n",
            "Val Loss: 9646.6141\n",
            "Epoch 33 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 67.76it/s]                         \n",
            "157it [00:01, 97.20it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 9643.6965\n",
            "Val Loss: 9632.3237\n",
            "Epoch 34 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 67.89it/s]                         \n",
            "157it [00:01, 99.97it/s]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 9637.2701\n",
            "Val Loss: 9653.0826\n",
            "Epoch 35 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 67.41it/s]\n",
            "157it [00:02, 70.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 9629.1247\n",
            "Val Loss: 9635.7212\n",
            "Epoch 36 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 67.35it/s]                         \n",
            "157it [00:01, 97.62it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 9623.4335\n",
            "Val Loss: 9617.3515\n",
            "Epoch 37 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 67.24it/s]                         \n",
            "157it [00:01, 99.49it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 9615.3487\n",
            "Val Loss: 9619.2439\n",
            "Epoch 38 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:13, 67.21it/s]                         \n",
            "157it [00:01, 86.53it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 9608.8956\n",
            "Val Loss: 9611.8225\n",
            "Epoch 39 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:14, 64.72it/s]                         \n",
            "157it [00:01, 97.77it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 9606.1454\n",
            "Val Loss: 9618.8427\n",
            "Epoch 40 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 53%|    | 496/937 [00:06<00:06, 70.03it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yl2icHjRr2q7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}